---
title: "Faster and Scalable Credit Risk Prediction"
author: "Fang Zhou, Data Scientist, Microsoft"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE, purl=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.width = 8,
                      fig.height = 5,
                      fig.align='center',
                      dev = "png")
```

## 1 Introduction

Microsoft R is a collection of servers and tools that extend the capabilities of R, making it easier and faster to build and deploy R-based solutions. Microsoft R brings you the ability to do parallel and chunked data processing and modelling that relax the restrictions on dataset size imposed by in-memory open source R. 

The `MicrosoftML` package brings new machine learning functionality with increased speed, performance and scalability, especially for handling a large corpus of text data or high-dimensional categorical data. The `MicrosoftML` package is installed with **Microsoft R Client**, **Microsoft R Server** and with the **SQL Server Machine Learning Services**.

This document will walk through you how to build faster and scalable credit risk models, using the `MicrosoftML` package that adds state-of-the-art machine learning algorithms and data transforms to Microsoft R Server.

## 2 Faster and Scalable Credit Risk Models

### 2.1 Setup

We load the required R packages.

```{r, message=FALSE, warning=FALSE, error=FALSE}
## Setup

# Load the required packages into the R session.

library(rattle)       # Use normVarNames().
library(dplyr)        # Wrangling: tbl_df(), group_by(), print(), glimpse().
library(magrittr)     # Pipe operator %>% %<>% %T>% equals().
library(scales)       # Include commas in numbers.
library(RevoScaleR)   # Enable out-of-memory computation in R.
library(dplyrXdf)     # Wrangling on xdf data format.
library(MicrosoftML)  # Build models using Microsoft ML algortihms.
library(caret)        # Calculate confusion matrix by using confusionMatrix().
library(ROCR)         # Provide functions for model performance evaluation.
```

Then, the dataset processedSimu is ingested and transformed into a `.xdf` data format. This dataset was created by the data preprocessing steps in the data science accelerator for credit risk prediction.

```{r, message=FALSE, warning=FALSE, error=FALSE}
## Data Ingestion

# Identify the source location of the dataset.

#DATA <- "../../Data/"
#data_fname <- file.path(DATA, "Raw/processedSimu.csv")

wd <- getwd()

dpath <- "../Data"
data_fname <- file.path(wd, dpath, "processedSimu.csv")
output_fname <- file.path(wd, dpath, "processedSimu.xdf")
output <- RxXdfData(file=output_fname)

# Ingest the dataset.

data <- rxImport(inData=data_fname, 
                 outFile=output,
                 stringsAsFactors=TRUE,
                 overwrite=TRUE)


# View data information.

rxGetVarInfo(data)
```

### 2.2 Model Building

Now, let's get started to build credit risk models by leveraging different machine learning algorithms from the `MicrosoftML` package. 

First of all, we create individual machine learning models on the dataset processedSimu by using the functions `rxLogisticRegression()`, `rxFastTrees()`, and `rxFastForest()`. 

```{r, message=FALSE, warning=FALSE, error=FALSE}
## Variable roles.

# Target variable

target <- "bad_flag"

# Note any identifier.

id <- c("account_id") %T>% print() 

# Note the available variables as model inputs.

vars <- setdiff(names(data), c(target, id))
```

```{r, message=FALSE, warning=FALSE, error=FALSE}
# Split Data

set.seed(42)

train <- sample(nrow(data), 0.70 * nrow(data))
test <- setdiff(seq_len(nrow(data)), train)

# Add training/testing flag to each observation.

data %<>%
  mutate(.train=factor(sample(1:2, .rxNumRows,
                              replace=TRUE,
                              prob=c(0.70, 0.30)),
                       levels=1:2))

# Split dataset into training/test.

data_split <- rxSplit(data, splitByFactor=".train")
```

```{r, message=FALSE, warning=FALSE, error=FALSE}
# Prepare the formula

top_vars <- c("amount_6", "pur_6", "avg_pur_amt_6", "avg_interval_pur_6", "credit_limit", "age", "income", "sex", "education", "marital_status")

form <- as.formula(paste(target, paste(top_vars, collapse="+"), sep="~"))
form
```

```{r, message=FALSE, warning=FALSE, error=FALSE}
# Specify the local parallel compute context.

rxSetComputeContext("localpar")

# Train model: rxLogisticRegression

time_rxlogit <- system.time(
  
  model_rxlogit <- rxLogisticRegression(
    formula=form,
    data=data_split[[1]],
    type="binary",
    l1Weight=1)
)

# Train model: rxFastTrees

time_rxtrees <- system.time(
  
  model_rxtrees <- rxFastTrees(
    formula=form,
    data=data_split[[1]],
    type="binary",
    numTrees=100,
    numLeaves=20,
    learningRate=0.2,
    minSplit=10)
)

# Train model: rxFastForest

time_rxforest <- system.time(
  
  model_rxforest <- rxFastForest(
    formula=form,
    data=data_split[[1]],
    type="binary",
    numTrees=100,
    numLeaves=20,
    minSplit=10)
)
```

Next, we build an ensemble of models by using the function `rxEnsemble()`.

```{r, message=FALSE, warning=FALSE, error=FALSE}
# Train an ensemble model.

time_ensemble <- system.time(
  
  model_ensemble <- rxEnsemble(
    formula=form,
    data=data_split[[1]],
    type="binary",
    trainers=list(logisticRegression(), fastTrees(), fastForest()),
    replace=TRUE
  )
)
```

### 2.3 Model Evaluation 

Finally, we evaluate and compare the above built models.

```{r, message=FALSE, warning=FALSE, error=FALSE}
# Predict

models <- list(model_rxlogit, model_rxtrees, model_rxforest, model_ensemble)

# Predict class

predictions <- lapply(models, 
                      rxPredict, 
                      data=data_split[[2]]) %>%
                lapply('[[', 1)

levels(predictions[[4]]) <- c("no", "yes")

# Confusion matrix evaluation results.

cm_metrics <-lapply(predictions,
                    confusionMatrix, 
                    reference=data_split[[2]][[target]],
                    positive="yes")

# Accuracy

acc_metrics <- 
  lapply(cm_metrics, `[[`, "overall") %>%
  lapply(`[`, 1) %>%
  unlist() %>%
  as.vector()

# Recall

rec_metrics <- 
  lapply(cm_metrics, `[[`, "byClass") %>%
  lapply(`[`, 1) %>%
  unlist() %>%
  as.vector()
  
# Precision

pre_metrics <- 
  lapply(cm_metrics, `[[`, "byClass") %>%
  lapply(`[`, 3) %>%
  unlist() %>%
  as.vector()

# Predict class probability

probs <- lapply(models[c(1, 2, 3)],
                rxPredict,
                data=data_split[[2]]) %>%
                lapply('[[', 3)

# Create prediction object

preds <- lapply(probs, 
                ROCR::prediction,
                labels=data_split[[2]][[target]])

# Auc

auc_metrics <- lapply(preds, 
                      ROCR::performance,
                      "auc") %>%
               lapply(slot, "y.values") %>%
               lapply('[[', 1) %>%
               unlist()

auc_metrics <- c(auc_metrics, NaN)

algo_list <- c("rxLogisticRegression", "rxFastTrees", "rxFastForest", "rxEnsemble")
time_consumption <- c(time_rxlogit[3], time_rxtrees[3], time_rxforest[[3]], time_ensemble[3])

df_comp <- 
  data.frame(Models=algo_list, 
             Accuracy=acc_metrics, 
             Recall=rec_metrics, 
             Precision=pre_metrics,
             AUC=auc_metrics,
             Time=time_consumption) %T>%
             {head(.) %>% print()}
```

